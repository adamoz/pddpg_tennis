{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation in the Environment\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pddpg_tennis.environment import UnityEnvironmentWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "reacher_env = UnityEnvironmentWrapper(env_binary='../bin/tennis/Tennis.x86_64', train_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains 2 agents that control tennis rackets.  At each time step, agents control 2 continuous actions, move forward/backward & up/down with a racekt.  \n",
    "\n",
    "The state space has `24` dimensions.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 agents\n",
      "Number of actions: 2\n",
      "States look like:\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -7.38993645 -1.5        -0.          0.\n",
      "  6.83172083  5.99607611 -0.          0.        ]\n",
      "States have length: 24\n"
     ]
    }
   ],
   "source": [
    "starting_states = reacher_env.reset()\n",
    "\n",
    "print(f'We have {reacher_env.num_agents} agents')\n",
    "\n",
    "action_size = reacher_env.action_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "print('States look like:\\n', starting_states[0])\n",
    "\n",
    "state_size = reacher_env.state_size\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score from run 0: 0.09000000357627869, [raw: [ 0.    0.09]]\n",
      "Score from run 1: 0.0, [raw: [-0.01  0.  ]]\n",
      "Score from run 2: 0.0, [raw: [ 0.   -0.01]]\n",
      "Score from run 3: 0.0, [raw: [-0.01  0.  ]]\n",
      "Score from run 4: 0.0, [raw: [ 0.   -0.01]]\n",
      "Score from run 5: 0.0, [raw: [ 0.   -0.01]]\n",
      "Score from run 6: 0.0, [raw: [ 0.   -0.01]]\n",
      "Score from run 7: 0.10000000149011612, [raw: [-0.01  0.1 ]]\n",
      "Score from run 8: 0.0, [raw: [ 0.   -0.01]]\n",
      "Score from run 9: 0.0, [raw: [-0.01  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_agents = reacher_env.num_agents\n",
    "action_size = reacher_env.action_size\n",
    "\n",
    "for run in range(10):\n",
    "    states = reacher_env.reset()                                  # reset env\n",
    "    scores = np.zeros(num_agents)                                 # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size)        # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                         # all actions between -1 and 1   \n",
    "\n",
    "        next_states, rewards, dones = reacher_env.step(actions)\n",
    "        scores += rewards                               \n",
    "        states = next_states\n",
    "\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "    print(f\"Score from run {run}: {np.max(scores)}, [raw: {scores}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reacher_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
